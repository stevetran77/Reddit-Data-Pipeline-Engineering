# OpenAQ Data Pipeline Engineering

An end-to-end data engineering pipeline designed to extract, load and transform air quality data from the **OpenAQ API v3**. 
The system uses Apache Airflow for orchestration, AWS S3 for data lake storage, AWS Glue (PySpark) for transformation, Amazon Athena for querying, then visualize on Looker Studio (connected by OWOX)

## ğŸ— Architecture

The end-to-end data pipeline architecture is as follows:

```
               Airflow (Docker Local)
         â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
         DAG:
           - openaq_to_athena_pipeline

                 â”‚
                 â–¼
        AWS Lambda: openaq-fetcher
        - BÆ°á»›c 1: /v3/locations (VN)
        - BÆ°á»›c 2: /v3/locations/{location_id}/measurements
        - Ghi raw JSON.gz vÃ o S3 (aq_raw/)

                 â”‚
                 â–¼
      S3 Bucket: openaq-data-pipeline
      â”œâ”€â”€ aq_raw/    (raw zone: JSON.gz tá»« API)
      â”œâ”€â”€ aq_dev/    (dev zone: Parquet, test ETL)
      â””â”€â”€ aq_prod/   (prod zone: Parquet, production)

                 â”‚
                 â–¼
               AWS Glue
      â”œâ”€â”€ Glue Jobs (dev/prod)
      â”‚   - Äá»c tá»« aq_raw/
      â”‚   - Ghi Parquet vÃ o aq_dev/ vÃ  aq_prod/
      â””â”€â”€ Glue Crawlers (raw/dev/prod)
          - Cáº­p nháº­t Glue Data Catalog

                 â”‚
                 â–¼
         Glue Data Catalog
      â”œâ”€â”€ raw_db   (tables mapping aq_raw/)
      â”œâ”€â”€ dev_db   (tables mapping aq_dev/)
      â””â”€â”€ prod_db  (tables mapping aq_prod/)

                 â”‚
                 â–¼
            Amazon Athena
      â”œâ”€â”€ database: aq_dev
      â”‚   â””â”€â”€ table: vietnam
      â””â”€â”€ database: aq_prod
          â””â”€â”€ table: vietnam

                 â”‚
                 â–¼
                 OWOX
      â”œâ”€â”€ source: aq_dev.vietnam
      â””â”€â”€ source: aq_prod.vietnam

                 â”‚
                 â–¼
            Looker Studio
      â”œâ”€â”€ dataset: aq_dev.vietnam
      â””â”€â”€ dataset: aq_prod.vietnam
```

### S3 Data Lake Structure

The project utilizes a 3-zone architecture in S3:

1. **`aq_raw/`**: Immutable raw JSON data fetched directly from the OpenAQAPI (gzip compressed).
2. **`aq_dev/`**: Development zone for transformed Parquet files (partitioned by Year/Month/Day).
3. **`aq_prod/`**: Production zone for stable, reporting-ready data.

## ğŸ”„ Data Flow

1. **Extraction**: Airflow triggers Python scripts to fetch location metadata and sensor measurements from OpenAQ API v3.
2. **Load**: Raw data is uploaded to S3 (`aq_raw`) in NDJSON format.
3. **Transformation**: An **AWS Glue PySpark job** is triggered. It:
   - Reads raw JSON files from `aq_raw`.
   - Cleanses and transforms the data (e.g., parsing dates, handling missing values).
   - Partitions the data by Year/Month/Day for efficient querying.
4. **Loading**: Transformed data is written to S3 (`aq_dev` or `aq_prod`) as partitioned Parquet files.
5. **Serving**: AWS Glue Crawler updates the Data Catalog, making the data queryable via Amazon Athena.

## ğŸ›  Tech Stack

* **Language:** Python 3.11
* **Orchestration:** Apache Airflow 2.7.1 (Dockerized)
* **Containerization:** Docker & Docker Compose
* **Cloud Provider:** AWS
* **Storage:** Amazon S3
* **Processing:** AWS Glue (PySpark 3.4.1), Pandas
* **Query Engine:** Amazon Athena
* **Infrastructure:** Manually provisioned (bucket, roles) via AWS Console.

## ğŸ“‹ Prerequisites

1. **Docker Desktop** installed (with at least 4GB RAM allocated).
2. **AWS Account** with permissions for S3, Glue, and Athena.
3. **OpenAQ API Key** (Get one at [openaq.org](https://openaq.org/)).

## ğŸš€ Installation & Setup

### 1. Clone the Repository

```bash
git clone https://github.com/stevetran77/OpenAQ-Data-Pipeline-Engineering.git
cd OpenAQ-Data-Pipeline-Engineering

```

### 2. Configure AWS Resources

Follow the guide in `doc/AWS_CONFIG_GUIDE.md` to set up:

* S3 Bucket (e.g., `openaq-data-pipeline`)
* Glue Database (e.g., `openaq_dev`)
* IAM Roles for Glue and Athena

### 3. Set up Configuration

Create the configuration file from the example:

```bash
# Linux/Mac
cp config/config.conf.example config/config.conf

# Windows
copy config\config.conf.example config\config.conf

```

**Edit `config/config.conf**` and fill in your details:

* `[aws]`: Access Key, Secret Key, Region, Bucket Name.
* `[aws_glue]`: IAM Role ARN (from Step 2).
* `[api_keys]`: Your OpenAQ API Key.

### 4. Build and Start Docker

```bash
docker-compose build
docker-compose up -d

```

* **Airflow Webserver:** [http://localhost:8080](https://www.google.com/search?q=http://localhost:8080)
* **Username/Password:** `admin` / `admin`

## ğŸƒ Running the Pipeline

1. Access the Airflow UI at `http://localhost:8080`.
2. Locate the DAG named **`openaq_to_athena_pipeline`**.
3. Toggle the switch to **Unpause** the DAG.
4. Click the **Trigger DAG** (Play button) to start a manual run.

**The DAG performs the following steps:**

1. `extract_all_vietnam_locations`: Fetches data for all VN sensors.
2. `trigger_glue_transform_job`: Offloads processing to AWS Glue (Spark).
3. `wait_glue_transform_job`: Sensors the Glue job status.
4. `trigger_glue_crawler`: Catalogs the new Parquet files.
5. `validate_athena_data`: Checks if data is queryable.

## ğŸ§ª Testing

You can run unit tests and integration tests inside the Docker container or locally.

```bash
# Run the full test suite
pytest tests/

# Test specific components
pytest tests/test_extract_data.py  # Test API extraction
pytest tests/test_glue_complete.py # Test PySpark logic locally
pytest tests/test_athena_connection.py # Validate AWS Connectivity

```

## ğŸ“‚ Project Structure

```bash
.
â”œâ”€â”€ config/                 # Configuration files (gitignored)
â”œâ”€â”€ dags/                   # Airflow DAG definitions
â”‚   â”œâ”€â”€ tasks/              # Task factories (extract, catalog, validate)
â”‚   â””â”€â”€ openaq_dag.py       # Main DAG file
â”œâ”€â”€ doc/                    # Documentation (Architecture, Guides)
â”œâ”€â”€ etls/                   # Logic for OpenAQ API extraction
â”œâ”€â”€ glue_jobs/              # PySpark scripts uploaded to AWS Glue
â”‚   â””â”€â”€ process_openaq_raw.py
â”œâ”€â”€ pipelines/              # High-level pipeline orchestration logic
â”œâ”€â”€ tests/                  # Unit and integration tests
â”œâ”€â”€ utils/                  # Shared utilities (AWS, Constants, Logging)
â”œâ”€â”€ Dockerfile              # Custom Airflow image with PySpark/Java
â”œâ”€â”€ docker-compose.yml      # Container orchestration
â””â”€â”€ requirements.txt        # Python dependencies

```

## ğŸ“œ License

Distributed under the MIT License. See `LICENSE` for more information.

---

*Author: [Steve Tran*](https://www.google.com/search?q=https://github.com/stevetran77)